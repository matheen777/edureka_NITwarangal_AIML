{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ACTION_UP    =  0\n",
    "ACTION_RIGHT =  1\n",
    "ACTION_DOWN  =  2\n",
    "ACTION_LEFT  =  3\n",
    "class GridWorld(object):\n",
    "    def __init__(self,shape=[5,5]):\n",
    "        self.shape = shape\n",
    "        numStates  = shape[0] * shape[1]\n",
    "        numActions = 4\n",
    "        self.numStates = numStates\n",
    "        self.numActions = numActions\n",
    "        xmax = shape[0]\n",
    "        ymax = shape[1]\n",
    "        grid = np.arange(numStates).reshape(shape)\n",
    "        Model = {}\n",
    "        x_indices = np.arange(xmax)\n",
    "        y_indices = np.arange(ymax)\n",
    "        for x in x_indices:\n",
    "            for y in y_indices:\n",
    "                state = y + x*(xmax)\n",
    "                Model[state] ={action:[] for action in np.arange(numActions)}\n",
    "                is_terminal_state = lambda state : state == 0 or state == (numStates-1)\n",
    "                reward = 0.0 if is_terminal_state(state) else -1.0\n",
    "                if is_terminal_state(state):\n",
    "                    Model[state][ACTION_UP] = [(1.0,state,reward,True)]\n",
    "                    Model[state][ACTION_RIGHT] = [(1.0,state,reward,True)]\n",
    "                    Model[state][ACTION_DOWN] = [(1.0,state,reward,True)]\n",
    "                    Model[state][ACTION_LEFT] = [(1.0,state,reward,True)]\n",
    "                else:\n",
    "                    next_state = {}\n",
    "                    next_state[ACTION_UP] = state if x == 0 else state - ymax\n",
    "                    next_state[ACTION_RIGHT] = state if y == ymax-1 else state +1\n",
    "                    next_state[ACTION_DOWN] = state if x == xmax-1 else state + ymax\n",
    "                    next_state[ACTION_LEFT] = state if y == 0 else state -1\n",
    "                    Model[state][ACTION_UP] = [(1.0,next_state[ACTION_UP] ,reward,is_terminal_state(next_state[ACTION_UP]))]\n",
    "                    Model[state][ACTION_RIGHT] = [(1.0,next_state[ACTION_RIGHT],reward,is_terminal_state(next_state[ACTION_RIGHT]))]\n",
    "                    Model[state][ACTION_DOWN] = [(1.0,next_state[ACTION_DOWN],reward,is_terminal_state(next_state[ACTION_DOWN]))]\n",
    "                    Model[state][ACTION_LEFT] = [(1.0,next_state[ACTION_LEFT],reward,is_terminal_state(next_state[ACTION_LEFT]))]\n",
    "        self.model = Model\n",
    "\n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "# Helper function to calculate the value for all actions in a given state    \n",
    "    def compute_value_fn_update(state,value_fn):\n",
    "        value_fn_update = np.zeros(env.numActions)\n",
    "        for action in range(env.numActions):\n",
    "            for prob,next_state,reward,done in env.model[state][action]:\n",
    "                value_fn_update[action] += prob * (reward + discount_factor * value_fn[next_state])\n",
    "               \n",
    "        return value_fn_update\n",
    "   \n",
    "    value_fn = np.zeros(env.numStates)\n",
    "    while True:\n",
    "# Stopping Condition        \n",
    "        delta = 0\n",
    "# Update each state        \n",
    "        for state in range(env.numStates):\n",
    "# Find the best action\n",
    "            action_values = compute_value_fn_update(state, value_fn)\n",
    "            best_action_value = np.max(action_values)\n",
    "# Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - value_fn[state]))\n",
    "# Update the value function\n",
    "            value_fn[state] = best_action_value        \n",
    "# Check if we can stop      \n",
    "        if delta < theta:\n",
    "            break\n",
    "   \n",
    "    # Create a deterministic policy by using the optimal value function\n",
    "    policy = np.zeros([env.numStates, env.numActions])\n",
    "    for state in range(env.numStates):\n",
    "    # Find the best action for this state\n",
    "        A = compute_value_fn_update(state, value_fn)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[state, best_action] = 1.0\n",
    "   \n",
    "    return policy, value_fn\n",
    "       \n",
    "       \n",
    "env = GridWorld()\n",
    "policy, value_fn = value_iteration(env)\n",
    "result = np.reshape(np.argmax(policy, axis=1), env.shape)\n",
    "ans1=str(result[0][0]) + ',' + str(result[0][1]) + ','+ str(result[0][2]) + ','+ str(result[0][3]) + ','+ str(result[0][4])\n",
    "ans2=str(result[1][0]) + ',' + str(result[1][1]) + ','+ str(result[1][2]) + ','+ str(result[1][3]) + ','+ str(result[1][4])\n",
    "ans3=str(result[2][0]) + ',' + str(result[2][1]) + ','+ str(result[2][2]) + ','+ str(result[2][3]) + ','+ str(result[2][4])\n",
    "ans4=str(result[3][0]) + ',' + str(result[3][1]) + ','+ str(result[3][2]) + ','+ str(result[3][3]) + ','+ str(result[3][4])\n",
    "ans5=str(result[4][0]) + ',' + str(result[4][1]) + ','+ str(result[4][2]) + ','+ str(result[4][3]) + ','+ str(result[4][4])\n",
    "\n",
    "result=[ans1, ans2, ans3,ans4, ans5]\n",
    "st=''\n",
    "for i in (result):\n",
    "    st = st +str(i)+'\\n'\n",
    "filename = \"/code/output/output.csv\"\n",
    "result = open(filename,\"w\")\n",
    "result.write(st)\n",
    "result.close\n",
    "result = open(filename,\"r\")\n",
    "print(result.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
